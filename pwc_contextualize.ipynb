{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc320f59-388f-4fed-b628-68b59e7a2ce6",
   "metadata": {},
   "source": [
    "# This is an RAG Chatbot that can answer questions based on an article about Vendée Globe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efee56-ab20-4819-b279-550d4c8c4637",
   "metadata": {},
   "source": [
    "## This bot is using two models: one the contextualize the user question, and one to answer it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205c606-2150-427f-9dbe-ab087635623b",
   "metadata": {},
   "source": [
    "### Imports and warnings off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8743ac9-6d0d-4a40-a990-2c71d627480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.schema import Document  \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.schema import AIMessage, HumanMessage \n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a45eea-ef2e-487c-9db3-03cfd6b964d7",
   "metadata": {},
   "source": [
    "### Scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a6084fa-55cb-4613-aa88-04ca3e4b9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = \" \".join([para.get_text() for para in paragraphs])\n",
    "        return content.strip()\n",
    "    else:\n",
    "        print(f\"Failed: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da939263-89fc-4d00-8e62-7359f6bf204d",
   "metadata": {},
   "source": [
    "### Initializing FAISS Database and Q&A pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59478329-fb8c-4e7c-9717-6dcaa81d286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_db_q(content, url, model_path, model_name):\n",
    "    documents = [Document(page_content=content, metadata={\"source\": url})]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    model_kwargs = {'device': 'cpu'} \n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "    qa_pipeline_instance = pipeline(\n",
    "        \"question-answering\",\n",
    "        model=model_name,\n",
    "        tokenizer=tokenizer,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    return retriever, qa_pipeline_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a607142f-0911-4364-9207-a071bc6c1caf",
   "metadata": {},
   "source": [
    "### Chatbot class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce3cd20-7afe-455b-9543-c102b35dddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, retriever, qa_pipeline, memory_length=10, prompt_template=\"Hello! I am your assistant. {context}\"):\n",
    "        self.retriever = retriever\n",
    "        self.qa_pipeline = qa_pipeline\n",
    "        self.memory = ConversationBufferWindowMemory(k=memory_length)\n",
    "        self.prompt_template = prompt_template\n",
    "        self.prompt = self.prompt_template.format(context=\"How can I help you today?\")\n",
    "        self.memory.chat_memory.add_message(AIMessage(content=self.prompt))\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to initialize FLAN-T5: {e}\")\n",
    "\n",
    "    def generate_standalone_question(self, user_question):\n",
    "        history = self.memory.load_memory_variables({}).get('history', \"\")\n",
    "        if isinstance(history, list):\n",
    "            history_text = \"\\n\".join(history)\n",
    "        else:\n",
    "            history_text = history\n",
    " \n",
    "        input_text = (\n",
    "        f\"You are an intelligent assistant trained to rewrite questions. \"\n",
    "        f\"Your task is to take the given conversation history and the current question, \"\n",
    "        f\"and rewrite the question so it can be understood on its own without needing the history.\\n\\n\"\n",
    "        f\"- Do not answer the question.\\n\"\n",
    "        f\"- Use the conversation history to provide context.\\n\"\n",
    "        f\"- Ensure the rewritten question is clear, concise, and standalone.\\n\\n\"\n",
    "        f\"### Example:\\n\"\n",
    "        f\"History:\\n\"\n",
    "        f\"User: Where is the Vendée Globe happening?\\n\"\n",
    "        f\"Bot: The Vendée Globe is held in Les Sables-d'Olonne, France.\\n\"\n",
    "        f\"Current Question:\\n\"\n",
    "        f\"When does it start?\\n\"\n",
    "        f\"Rewritten Standalone Question:\\n\"\n",
    "        f\"When does the Vendée Globe in Les Sables-d'Olonne, France, start?\\n\\n\"\n",
    "        f\"---\\n\\n\"\n",
    "        f\"### Your Task:\\n\"\n",
    "        f\"History:\\n{history_text}\\n\\n\"\n",
    "        f\"Current Question:\\n{user_question}\\n\\n\"\n",
    "        f\"Rewritten Standalone Question:\"\n",
    "        )\n",
    "\n",
    "        \n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        outputs = self.model.generate(inputs.input_ids, max_length=100, num_beams=5)\n",
    "        standalone_question = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return standalone_question\n",
    "\n",
    "    def ask(self, question):\n",
    "        standalone_question = self.generate_standalone_question(question)\n",
    "        print(f\"Standalone Question: {standalone_question}\")\n",
    "\n",
    "        search_docs = self.retriever.invoke(standalone_question)\n",
    "        context = \"\\n\".join([doc.page_content for doc in search_docs])\n",
    "\n",
    "        try:\n",
    "            answer = self.qa_pipeline(question=standalone_question, context=context)\n",
    "\n",
    "            self.memory.chat_memory.add_message(HumanMessage(content=standalone_question))\n",
    "            self.memory.chat_memory.add_message(AIMessage(content=answer['answer']))\n",
    "\n",
    "            return answer['answer']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    def print_memory(self):\n",
    "        return self.memory.load_memory_variables({})['history']\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.memory.clear()\n",
    "        self.memory.chat_memory.add_message(AIMessage(content=self.prompt))\n",
    "\n",
    "    def chat(self):\n",
    "        print(self.prompt)\n",
    "        print(\"Type 'exit' to end the chat.\")\n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() == 'exit':\n",
    "                print(\"Chat ended.\")\n",
    "                break\n",
    "            response = self.ask(user_input)\n",
    "            print(f\"Bot: {response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b7b0dd-f156-4ab2-a0b4-14c463241d66",
   "metadata": {},
   "source": [
    "### Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9474264-a7bc-4fd9-8afc-351dba2df22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use `chatbot.chat()` to start a conversation,\n",
      "`chatbot.print_memory()` to view memory, and\n",
      "`chatbot.reset_memory()` to clear memory.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.yachtingworld.com/all-latest-posts/who-will-win-the-2024-vendee-globe-155320\"\n",
    "content =content_from_url(url)\n",
    "\n",
    "if content:\n",
    "    model_path = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "    model_name = \"Intel/dynamic_tinybert\"\n",
    "    \n",
    "    retriever, qa_pipeline_instance = init_db_q(content, url, model_path, model_name)\n",
    "    \n",
    "    chatbot = Chatbot(\n",
    "        retriever, \n",
    "        qa_pipeline_instance, \n",
    "        memory_length=5, \n",
    "        prompt_template=\"Hi! I am your vendee globe article bot. {context}\"\n",
    "    )\n",
    "    \n",
    "    print(\"Use `chatbot.chat()` to start a conversation,\\n`chatbot.print_memory()` to view memory, and\\n`chatbot.reset_memory()` to clear memory.\")\n",
    "else:\n",
    "    print(\"Failed to fetch url content.\")\n",
    "\n",
    "chatbot = Chatbot(\n",
    "    retriever, \n",
    "    qa_pipeline_instance, \n",
    "    memory_length=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7890c3-570b-4d1f-acee-cb7d1a794df0",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f316ba-9a64-4053-9919-ffcc6b0e65cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am your assistant. How can I help you today?\n",
      "Type 'exit' to end the chat.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  who is the captain of macif?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standalone Question: Who is the captain of macif?\n",
      "Bot: Charlie Dalin\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "chatbot.chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
